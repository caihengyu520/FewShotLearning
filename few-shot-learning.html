<html>
<head>
  <title>Evernote Export</title>
  <basefont face="微软雅黑" size="2" />
  <meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
  <meta name="exporter-version" content="YXBJ Windows/600667 (zh-CN, DDL); Windows/10.0.0 (Win64);"/>
  <style>
    body, td {
      font-family: 微软雅黑;
      font-size: 10pt;
    }
  </style>
</head>
<body>
<a name="392"/>

<div>
<span><div>未来我们会如何，会走向何方</div></span>
</div>
<hr>
<a name="497"/>

<div>
<span><div><a href="few-shot-learning_files/小样本学习xiugai.pptx"><img src="few-shot-learning_files/4aa234a68a18231f0c3b621fc7619288.png" alt="小样本学习xiugai.pptx"></a></div><div><a href="few-shot-learning_files/Learning to Compare Relation Network for Few-Shot Learning.pdf"><img src="few-shot-learning_files/759934061c894f526168595d272e50b0.png" alt="Learning to Compare Relation Network for Few-Shot Learning.pdf"></a></div><div><br/></div></span>
</div>
<hr>
<a name="673"/>

<div>
<span><div>孪生网络采取一种绝对值距离用来衡量不同样本之间的相似度，损失函数采用二分类交叉熵损失函数为例前来优化</div><div>采用绝对值距离度量即衡量了不同样本之间的距离时考虑了量纲，如果采用cosine损失函数则等于仅仅考虑了夹角，而未考虑量纲大小，即单位向量之间的夹角</div><div>并且在绝对值距离计算时学习了权重，即对样本不同分量之间的相似性重要性进行自学习</div><div>超参数优化采用贝叶斯超参数优化进行实现，贝叶斯优化核心就是不断用先验知识去预测后验知识，然后再将后验知识当成先验知识去预测后面的步骤</div><div>具体的学习过程结合了one-shot和siamese network，结合了深度网络，权重初始化采用正态分布，采用了仿射变换进行数据增强</div><div>在该论文之后，又出现了siamese network用于图像追踪的论文，并取得了不错的成果</div><div><br/></div><div><br/></div><div>孪生网络训练起来较为简单，采取一对样本送进网络，参数的更新是同时考虑两个样本之间的关系，区别于原型网络，原型网络是事先确定聚类中心，以样本和聚类中心进行比较，但是其缺点在于推断时需要将测试样本和支持集进行一一判别，这样的话比较耗费计算力</div><div><br/></div><div><img src="few-shot-learning_files/Image.png" type="image/png" data-filename="Image.png"/></div><div><br/></div></span>
</div>
<hr>
<a name="675"/>

<div>
<span><div>原型网络主要是将聚类思想结合进小样本学习中，并且将其与match network进行了对比分析</div><div>圆形网络通过将支持集进行嵌入聚类，提取特征后得出类中心向量， <span style="font-size: 16px; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; color: rgb(79, 79, 79); font-family: &quot;Microsoft YaHei&quot;, &quot;SF Pro Display&quot;, Roboto, Noto, Arial, &quot;PingFang SC&quot;, sans-serif; font-variant-caps: normal; font-variant-ligatures: normal;">原形网络将每个类别中的样例数据映射到一个空间当中，并且提取他们的“均值”来表示为该类的原形（prototype）。然后将测试数据集与原型中心进行比较，从而得出其类别，距离采用的是欧几里得距离，符合bregman divergence性质的距离。最后进行损失计算的时候，采用一种逻辑斯蒂回归，概率最大化思想，通过加上负号得出损失函数最小化思想。选用某种距离度量实际上限定了在嵌入空间中类条件数据分布的建模假设！</span></div><div><span style="font-size: 16px; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;"><br/></span></div><div><span style="font-size: 16px; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; color: rgb(79, 79, 79); font-family: &quot;Microsoft YaHei&quot;, &quot;SF Pro Display&quot;, Roboto, Noto, Arial, &quot;PingFang SC&quot;, sans-serif; font-variant-caps: normal; font-variant-ligatures: normal;">该论文给我的思想就是类似于孪生网络，但是明显采用了一种聚类思想，提前确定类中心，然后通过对嵌入网络进行修改调整聚类中心使其适应于新的测试数据等。而孪生网络就是单独考虑测试样本和每一个支持样本之间的关系进行权重调整，这两者之间孰优孰劣值得思考？</span></div><div><span style="font-size: 16px; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; color: rgb(79, 79, 79); font-family: &quot;Microsoft YaHei&quot;, &quot;SF Pro Display&quot;, Roboto, Noto, Arial, &quot;PingFang SC&quot;, sans-serif; font-variant-caps: normal; font-variant-ligatures: normal;">另外，将原型网络进行调整为多元原型网络需要考虑一个分区操作和之后权重更新操作的解耦形式，这样大大增加计算复杂性（</span> <span style="font-size: 12pt; color: rgb(79, 79, 79); font-family: &quot;Microsoft YaHei&quot;;">Distance-Based Image Classification:</span><span style="font-size: 12pt; color: rgb(79, 79, 79); font-family: &quot;Microsoft YaHei&quot;;">Generalizing to New Classes </span><span style="font-size: 12pt; color: rgb(79, 79, 79); font-family: &quot;Microsoft YaHei&quot;;">at Near-Zero Cost</span><span style="font-size: 16px; color: rgb(79, 79, 79); font-family: &quot;Microsoft YaHei&quot;, &quot;SF Pro Display&quot;, Roboto, Noto, Arial, &quot;PingFang SC&quot;, sans-serif;">）</span></div><div><span style="font-size: 16px; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;"><br/></span></div><div><span style="font-size: 16px; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; color: rgb(79, 79, 79); font-family: &quot;Microsoft YaHei&quot;, &quot;SF Pro Display&quot;, Roboto, Noto, Arial, &quot;PingFang SC&quot;, sans-serif; font-variant-caps: normal; font-variant-ligatures: normal;">bregman散度是损失或失真函数，假设有x和y两个点，y是原来的点，而x是它的某个失真近似，x可能是由于添加了一些随机噪声到y上面所形成的，损失函数的母的就是度量用x近似y导致的失真或损失，当然，x和y越类似，失真或损失就越小，因而bregman散度可以用作相异性函数（可否考虑在计算相似性时结合一种权重机制，类似于siamese network时使用的，加入一定的权重机制，虽然增加复杂性，但是应该有利于提升模型性能，主要是考虑不同属性对相似性度量的重要性不一样）</span></div><div><span style="font-size: 12pt; color: rgb(79, 79, 79); font-family: &quot;Microsoft YaHei&quot;;">bregman度量和L1度量相比，等于在L1优化的基础上继续进一步优化</span></div><div><span style="font-size: 12pt; color: rgb(79, 79, 79); font-family: &quot;Microsoft YaHei&quot;;"><img src="few-shot-learning_files/Image [1].png" type="image/png" data-filename="Image.png"/></span></div></span>
</div>
<hr>
<a name="677"/>

<div>
<span><div>度量学习方法，最为简单的即一些传统的距离例如欧式距离，马氏距离，bregman散度等等</div><div>马氏距离（Mahalanobis Distance）是由印度统计学家马哈拉诺比斯（P. C. Mahalanobis）提出的，表示数据的协方差距离。它是一种有效的计算两个未知样本集的相似度的方法。与欧氏距离不同的是它考虑到各种特性之间的联系（例如：一条关于身高的信息会带来一条关于体重的信息，因为两者是有关联的）并且是尺度无关的（scale-invariant），即独立于测量尺度。马氏距离计算公式可以简化为对于一个均值为μ=(μ1,μ2,μ3,...,μp)Tμ=(μ1,μ2,μ3,...,μp)T，协方差矩阵为∑∑的多变量x=(x1,x2,x3,...,xp)Tx=(x1,x2,x3,...,xp)T，其马氏距离为：</div><div>DM(x)=(x−μ)T∑−1(x−μ)−−−−−−−−−−−−−−−−−√</div><div>DM(x)=(x−μ)T∑−1(x−μ)</div><div><br/></div><div>  如果∑−1∑−1是单位阵的时候，马氏距离简化为欧氏距离。</div><div><span style="color: rgb(79, 79, 79); font-family: &quot;Microsoft YaHei&quot;, &quot;SF Pro Display&quot;, Roboto, Noto, Arial, &quot;PingFang SC&quot;, sans-serif; font-size: 16px; font-variant-ligatures: common-ligatures; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;">马氏距离有很多优点，马氏距离不受量纲的影响，两点之间的马氏距离与原始数据的测量单位无关；由标准化数据和中心化数据(即原始数据与均值之差）计算出的二点之间的马氏距离相同。马氏距离还可以排除变量之间的相关性的干扰。</span><span style="color: rgb(79, 79, 79); font-family: &quot;Microsoft YaHei&quot;, &quot;SF Pro Display&quot;, Roboto, Noto, Arial, &quot;PingFang SC&quot;, sans-serif; font-size: 16px; font-variant-ligatures: common-ligatures; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;"> 但是马氏距离</span><span style="color: rgb(79, 79, 79); font-family: &quot;Microsoft YaHei&quot;, &quot;SF Pro Display&quot;, Roboto, Noto, Arial, &quot;PingFang SC&quot;, sans-serif; font-size: 16px; font-variant-ligatures: common-ligatures;">它的缺点是夸大了变化微小的变量的作用。</span></div><div><span style="color: rgb(79, 79, 79); font-family: &quot;Microsoft YaHei&quot;, &quot;SF Pro Display&quot;, Roboto, Noto, Arial, &quot;PingFang SC&quot;, sans-serif; font-size: 16px; font-variant-ligatures: common-ligatures;"><br/></span></div><div><span style="font-size: 12pt; color: rgb(79, 79, 79); font-family: &quot;Microsoft YaHei&quot;;">马氏距离的转化过程：在最后一步中，坐标轴扩展的量是协方差矩阵的逆的特征值（平方根），同理的，坐标轴缩小的量是协方差矩阵的特征值。所以，点越分散，需要的将椭圆转成圆的缩小量就越多。</span><span style="color: rgb(79, 79, 79); font-family: &quot;Microsoft YaHei&quot;; font-size: 12pt;">尽管上述的操作可以用到任何数据上，但是对于多元正态分布的数据表现更好。在其他情况下，点的平均值或许不能很好的表示数据的中心，或者数据的“脊椎”（数据的大致趋势方向）不能用变量作为概率分布测度（using variance as a measure of spread）来准确的确定。</span></div><div><span style="font-size: 12pt; color: rgb(79, 79, 79); font-family: &quot;Microsoft YaHei&quot;;"><br/></span></div><div><br/></div><div>现代的主要有结合深度学习（监督学习，无监督学习等等）以及线性度量，非线性度量等等，结合各种降维方法例如（LLE，LE，ISOMAP等）</div></span>
</div>
<hr>
<a name="679"/>

<div>
<span><div>在做分类的时候常常需要估算不同样本之间的相似性度量（similarity measurement），这是通常采用的方法就是计算样本间的距离，采用何种方法计算距离直接影响到分类正确与否</div><div>以下所述的各种距离基本上都可以用matlab操作实现</div><div><br/></div><div>1，欧式距离</div><div>欧氏空间中两点的距离</div><div><br/></div><div>2，曼哈顿距离</div><div>横坐标之差加上纵坐标之差</div><div><br/></div><div>3，切比雪夫距离</div><div>简单理解就是各维之中最大的距离</div><div><br/></div><div>4，闵科夫斯基距离</div><div>是一组距离公式，可以认为欧式距离，曼哈顿距离，切比雪夫距离都属于闵氏距离，问题在于并没有考虑各维之间不同的量纲</div><div><br/></div><div>5，标准欧式距离</div><div>其就是在计算距离之前先进行标准化，将各个分量转化为均值为0，方差为1的分量，以此将不同分量的量纲统一</div><div><br/></div><div>6，马氏距离</div><div>该距离的计算涉及到协方差矩阵的逆，并且该距离是量纲无关的，排除变量之间相关性的干扰</div><div><br/></div><div>7，余弦距离</div><div>余弦距离可以定义两个向量之间的相似性，两个向量之间越相似，其余弦值越相同， <span style="color: rgb(79, 79, 79); font-family: &quot;Microsoft YaHei&quot;, &quot;SF Pro Display&quot;, Roboto, Noto, Arial, &quot;PingFang SC&quot;, sans-serif; font-size: 14px; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;">夹角余弦取值范围为[-1,1]。夹角余弦越大表示两个向量的夹角越小，夹角余弦越小表示两向量的夹角越大。当两个向量的方向重合时夹角余弦取最大值1，当两个向量的方向完全相反夹角余弦取最小值-1。</span></div><div><span style="color: rgb(79, 79, 79); font-family: &quot;Microsoft YaHei&quot;, &quot;SF Pro Display&quot;, Roboto, Noto, Arial, &quot;PingFang SC&quot;, sans-serif; font-size: 14px; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;"><br/></span></div><div><span style="color: rgb(79, 79, 79); font-family: &quot;Microsoft YaHei&quot;, &quot;SF Pro Display&quot;, Roboto, Noto, Arial, &quot;PingFang SC&quot;, sans-serif; font-size: 14px; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;">8，汉明距离</span></div><div><span style="color: rgb(79, 79, 79); font-family: &quot;Microsoft YaHei&quot;, &quot;SF Pro Display&quot;, Roboto, Noto, Arial, &quot;PingFang SC&quot;, sans-serif; font-size: 14px; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;">该距离通常用于编码容错性，</span> <span style="color: rgb(79, 79, 79); font-family: &quot;Microsoft YaHei&quot;, &quot;SF Pro Display&quot;, Roboto, Noto, Arial, &quot;PingFang SC&quot;, sans-serif; font-size: 14px; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;">两个等长字符串s1与s2之间的汉明距离定义为将其中一个变为另外一个所需要作的最小替换次数。例如字符串“1111”与“1001”之间的汉明距离为2。</span></div><div><span style="color: rgb(79, 79, 79); font-family: &quot;Microsoft YaHei&quot;, &quot;SF Pro Display&quot;, Roboto, Noto, Arial, &quot;PingFang SC&quot;, sans-serif; font-size: 14px; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;">通常用不同的分量站总分量长度的比值定义</span></div><div><span style="color: rgb(79, 79, 79); font-family: &quot;Microsoft YaHei&quot;, &quot;SF Pro Display&quot;, Roboto, Noto, Arial, &quot;PingFang SC&quot;, sans-serif; font-size: 14px; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;"><br/></span></div><div><span style="font-size: 11pt; color: rgb(79, 79, 79); font-family: &quot;Microsoft YaHei&quot;;">9，杰卡德相似系数</span></div><div><span style="color: rgb(79, 79, 79); font-family: &quot;Microsoft YaHei&quot;, &quot;SF Pro Display&quot;, Roboto, Noto, Arial, &quot;PingFang SC&quot;, sans-serif; font-size: 14px; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;">两个集合A和B的交集元素在A，B的并集中所占的比例，称为两个集合的杰卡德相似系数，用符号J(A,B)表示。</span></div><div><span style="color: rgb(79, 79, 79); font-family: &quot;Microsoft YaHei&quot;, &quot;SF Pro Display&quot;, Roboto, Noto, Arial, &quot;PingFang SC&quot;, sans-serif; font-size: 14px; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;">在matlab实现时，是以不同向量占非全0向量比值作为结果</span></div><div><span style="color: rgb(79, 79, 79); font-family: &quot;Microsoft YaHei&quot;, &quot;SF Pro Display&quot;, Roboto, Noto, Arial, &quot;PingFang SC&quot;, sans-serif; font-size: 14px; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;">与下述定义有所不同</span></div><div style="box-sizing: border-box; outline: 0px; padding: 0px; margin: 0px 0px 16px; font-family: &quot;Microsoft YaHei&quot;, &quot;SF Pro Display&quot;, Roboto, Noto, Arial, &quot;PingFang SC&quot;, sans-serif; font-size: 16px; color: rgb(79, 79, 79); line-height: 26px; overflow-x: auto; overflow-wrap: break-word; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); border: 0px; list-style: none;"><span style="box-sizing: border-box; outline: 0px; font-family: &quot;Microsoft YaHei&quot;, &quot;SF Pro Display&quot;, Roboto, Noto, Arial, &quot;PingFang SC&quot;, sans-serif; font-size: 14px; color: rgb(79, 79, 79); line-height: 26px; overflow-x: auto; overflow-wrap: break-word; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); border: 0px;">p ：样本A与B都是1的维度的个数</span></div><div style="box-sizing: border-box; outline: 0px; padding: 0px; margin: 0px 0px 16px; font-family: &quot;Microsoft YaHei&quot;, &quot;SF Pro Display&quot;, Roboto, Noto, Arial, &quot;PingFang SC&quot;, sans-serif; font-size: 16px; color: rgb(79, 79, 79); line-height: 26px; overflow-x: auto; overflow-wrap: break-word; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); border: 0px; list-style: none;"><span style="box-sizing: border-box; outline: 0px; font-family: &quot;Microsoft YaHei&quot;, &quot;SF Pro Display&quot;, Roboto, Noto, Arial, &quot;PingFang SC&quot;, sans-serif; font-size: 14px; color: rgb(79, 79, 79); line-height: 26px; overflow-x: auto; overflow-wrap: break-word; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); border: 0px;">q ：样本A是1，样本B是0的维度的个数</span></div><div style="box-sizing: border-box; outline: 0px; padding: 0px; margin: 0px 0px 16px; font-family: &quot;Microsoft YaHei&quot;, &quot;SF Pro Display&quot;, Roboto, Noto, Arial, &quot;PingFang SC&quot;, sans-serif; font-size: 16px; color: rgb(79, 79, 79); line-height: 26px; overflow-x: auto; overflow-wrap: break-word; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); border: 0px; list-style: none;"><span style="box-sizing: border-box; outline: 0px; font-family: &quot;Microsoft YaHei&quot;, &quot;SF Pro Display&quot;, Roboto, Noto, Arial, &quot;PingFang SC&quot;, sans-serif; font-size: 14px; color: rgb(79, 79, 79); line-height: 26px; overflow-x: auto; overflow-wrap: break-word; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); border: 0px;">r ：样本A是0，样本B是1的维度的个数</span></div><div style="box-sizing: border-box; outline: 0px; padding: 0px; margin: 0px 0px 16px; font-family: &quot;Microsoft YaHei&quot;, &quot;SF Pro Display&quot;, Roboto, Noto, Arial, &quot;PingFang SC&quot;, sans-serif; font-size: 16px; color: rgb(79, 79, 79); line-height: 26px; overflow-x: auto; overflow-wrap: break-word; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); border: 0px; list-style: none;"><span style="box-sizing: border-box; outline: 0px; font-family: &quot;Microsoft YaHei&quot;, &quot;SF Pro Display&quot;, Roboto, Noto, Arial, &quot;PingFang SC&quot;, sans-serif; font-size: 14px; color: rgb(79, 79, 79); line-height: 26px; overflow-x: auto; overflow-wrap: break-word; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; border: 0px;">s ：样本A与B都是0的维度的个数</span></div><div style="box-sizing: border-box; outline: 0px; padding: 0px; margin: 0px 0px 16px; font-family: &quot;Microsoft YaHei&quot;, &quot;SF Pro Display&quot;, Roboto, Noto, Arial, &quot;PingFang SC&quot;, sans-serif; font-size: 16px; color: rgb(79, 79, 79); line-height: 26px; overflow-x: auto; overflow-wrap: break-word; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); border: 0px; list-style: none;"><span style="box-sizing: border-box; outline: 0px; font-family: &quot;Microsoft YaHei&quot;, &quot;SF Pro Display&quot;, Roboto, Noto, Arial, &quot;PingFang SC&quot;, sans-serif; font-size: 14px; color: rgb(79, 79, 79); line-height: 26px; overflow-x: auto; overflow-wrap: break-word; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); border: 0px;">那么样本A与B的杰卡德相似系数可以表示为：</span></div><div style="box-sizing: border-box; outline: 0px; padding: 0px; margin: 0px 0px 16px; font-family: &quot;Microsoft YaHei&quot;, &quot;SF Pro Display&quot;, Roboto, Noto, Arial, &quot;PingFang SC&quot;, sans-serif; font-size: 16px; color: rgb(79, 79, 79); line-height: 26px; overflow-x: auto; overflow-wrap: break-word; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); border: 0px; list-style: none;"><span style="box-sizing: border-box; outline: 0px; font-family: &quot;Microsoft YaHei&quot;, &quot;SF Pro Display&quot;, Roboto, Noto, Arial, &quot;PingFang SC&quot;, sans-serif; font-size: 14px; color: rgb(79, 79, 79); line-height: 26px; overflow-x: auto; overflow-wrap: break-word; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; border: 0px;">这里p+q+r可理解为A与B的并集的元素个数，而p是A与B的交集的元素个数。</span></div><div style="text-align: start;"><br/></div><div style="text-align: start;">10，相关距离和相关系数</div><div style="text-align: start;">相关系数通常用【1，-1】前来表示，绝对值越大则相关性越高，1则是正相关，-1则是负相关</div><div style="text-align: start;"><br/></div><div style="text-align: start;">11，信息熵</div><div style="box-sizing: border-box; outline: 0px; padding: 0px; margin: 0px 0px 16px; font-family: &quot;Microsoft YaHei&quot;, &quot;SF Pro Display&quot;, Roboto, Noto, Arial, &quot;PingFang SC&quot;, sans-serif; font-size: 16px; color: rgb(79, 79, 79); line-height: 26px; overflow-x: auto; overflow-wrap: break-word; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-align: start; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); border: 0px; list-style: none;"><span style="font-size: 14px;">信息熵是衡量分布的混乱程度或分散程度的一种度量。分布越分散(或者说分布越平均)，信息熵就越大。分布越有序（或者说分布越集中），信息熵就越小。</span></div><div style="text-align: start;">12，巴氏距离</div><div style="text-align: start;"><span style="color: rgb(51, 51, 51); font-family: verdana, &quot;ms song&quot;, Arial, Helvetica, sans-serif; font-size: 14px; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;">在统计中，Bhattacharyya距离测量两个离散或连续概率分布的相似性。它与衡量两个统计样品或种群之间的重叠量的Bhattacharyya系数密切相关。Bhattacharyya距离和Bhattacharyya系数以20世纪30年代曾在印度统计研究所工作的一个统计学家A. Bhattacharya命名。同时，Bhattacharyya系数可以被用来确定两个样本被认为相对接近的，它是用来测量中的类分类的可分离性。</span><br/></div><div style="text-align: start;"><span style="font-size: 11pt; color: rgb(51, 51, 51); font-family: verdana;"><img src="few-shot-learning_files/Image [2].png" type="image/png" data-filename="Image.png"/></span><br/></div><div style="text-align: start;"><span style="font-size: 11pt; color: rgb(51, 51, 51); font-family: verdana;"><img src="few-shot-learning_files/Image [3].png" type="image/png" data-filename="Image.png"/></span><br/></div><div style="text-align: start;"><span style="font-size: 11pt; color: rgb(51, 51, 51); font-family: verdana;"><img src="few-shot-learning_files/Image [4].png" type="image/png" data-filename="Image.png"/></span><br/></div></span>
</div>
<hr>
<a name="690"/>

<div>
<span><div>学习标签传播</div><div>将元学习结合进小样本学习并且提升学习准确率</div><div>In this paper, we propose Transductive</div><div>Propagation Network (TPN), a novel meta-learning framework for transductive</div><div>inference that classifies the entire test set at once to alleviate the low-data problem.</div><div>Specifically, we propose to learn to propagate labels from labeled instances to</div><div>unlabeled test instances, by learning a graph construction module that exploits the</div><div>manifold structure in the data. TPN jointly learns both the parameters of feature</div><div>embedding and the graph construction in an end-to-end manner. We validate TPN</div><div>on multiple benchmark datasets, on which it largely outperforms existing few-shot</div><div>learning approaches and achieves the state-of-the-art results.</div><div><br/></div><div><img src="few-shot-learning_files/Image [5].png" type="image/png" data-filename="Image.png"/></div><div>四部分组成，特征嵌入，图构建，标签传播，损失回传</div><div>在学习过程中，图构建时涉及到流形学习，需要选择合适的嵌入后的低维子空间，所以我们选择高斯分布结合欧式距离计算，</div><div>为了选择一个好的欧式距离的方差，我们将其通过神经网络习得，之后再采用欧式距离进行计算得到距离权重矩阵，习得距离权重之后，在选择K个最近邻样本将W给稀疏化，之后通过闭式解的方式解得最终将要输出的矩阵？ <span style="font-size: 14px; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; color: rgb(51, 51, 51); font-family: &quot;microsoft yahei&quot;; font-variant-caps: normal; font-variant-ligatures: normal;">拉普拉斯映射就是直接在低维下找到样本，使得所有样本保持原来的相似度，文中拉普拉斯正则化是通过对权重矩阵进行与对角矩阵相乘实现的。</span> <span style="color: rgb(79, 79, 79); font-family: &quot;Microsoft YaHei&quot;, &quot;SF Pro Display&quot;, Roboto, Noto, Arial, &quot;PingFang SC&quot;, sans-serif; font-size: 16px; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;">可以直观的感受到，正则化项实际上起到了限制参数w的“</span><span style="font-weight: bold; box-sizing: border-box; outline: 0px; overflow-wrap: break-word; color: rgb(255, 0, 0); font-family: &quot;Microsoft YaHei&quot;, &quot;SF Pro Display&quot;, Roboto, Noto, Arial, &quot;PingFang SC&quot;, sans-serif; font-size: 16px; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;">变化程度或变化幅值</span><span style="color: rgb(79, 79, 79); font-family: &quot;Microsoft YaHei&quot;, &quot;SF Pro Display&quot;, Roboto, Noto, Arial, &quot;PingFang SC&quot;, sans-serif; font-size: 16px; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;">”的作用，具体来说，它可以令w的任何一个分量相比较于剩余分量变化程度保持一致，不至于出现变化特别明显的分量。直接的作用就是防止模型“</span><span style="box-sizing: border-box; outline: 0px; font-family: &quot;Microsoft YaHei&quot;, &quot;SF Pro Display&quot;, Roboto, Noto, Arial, &quot;PingFang SC&quot;, sans-serif; overflow-wrap: break-word; font-size: 16px; font-variant-ligatures: normal; font-variant-caps: normal; font-weight: bold; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; background-color: rgb(255, 255, 255); color: rgb(255, 0, 0);">过拟合</span><span style="color: rgb(79, 79, 79); font-family: &quot;Microsoft YaHei&quot;, &quot;SF Pro Display&quot;, Roboto, Noto, Arial, &quot;PingFang SC&quot;, sans-serif; font-size: 16px; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;">”，提高了模型的泛化性能。</span></div><div><span style="color: rgb(79, 79, 79); font-family: &quot;Microsoft YaHei&quot;, &quot;SF Pro Display&quot;, Roboto, Noto, Arial, &quot;PingFang SC&quot;, sans-serif; font-size: 16px; font-variant-ligatures: normal; font-variant-caps: normal; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;"><br/></span></div><div><span style="font-size: 14px; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; color: rgb(51, 51, 51); font-family: &quot;microsoft yahei&quot;; font-variant-caps: normal; font-variant-ligatures: normal;">难点之一：</span></div><div><span style="font-size: 14px; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; color: rgb(51, 51, 51); font-family: &quot;microsoft yahei&quot;; font-variant-caps: normal; font-variant-ligatures: normal;">　1）拉普拉斯矩阵是一种图的矩阵表示。 </span></div><div><span style="font-size: 14px; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; color: rgb(51, 51, 51); font-family: &quot;microsoft yahei&quot;; font-variant-caps: normal; font-variant-ligatures: normal;">　2）拉普拉斯映射是在保持原流形数据相似度的情况下，直接降维到低维空间。 </span></div><div><span style="font-size: 14px; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; color: rgb(51, 51, 51); font-family: &quot;microsoft yahei&quot;; font-variant-caps: normal; font-variant-ligatures: normal;">　3）谱聚类是通过最小割，刚好借助了拉普拉斯映射的思想，从而用携带切割信息的特向来表征原流形数据，再去聚类。（相比于传统聚类，谱聚类更侧重于数据相似度信息的保留，更具有针对性，计算效率也更高） </span></div><div><span style="font-size: 11pt; color: rgb(51, 51, 51); font-family: &quot;microsoft yahei&quot;;">拉普拉斯矩阵，拉普拉斯算子通常用于图像边缘检测的一种滤波算子</span></div><div><span style="font-size: 14px; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px;"><br/></span></div><div>特征嵌入：四个卷积块，每个卷积块有64个3x3的卷积核</div><div>图构建阶段：该阶段需要利用一种距离度量来学校样本之间的相似性权重，而本论文并未人工指定需要的距离，而是通过神经网络自动学习距离，得出距离权重矩阵    假设样本的分布属于一种高斯分布，其方差的选择很关键，该阶段是通过神经网络对每一个样本学习一个scale长度参数，然后利用该scale值对变量进行标准化（scale即方差），之后采用欧几里得距离，然后在距离上采用正则化拉普拉斯。</div><div>标签传播：利用距离权重矩阵得出标签传播矩阵，该标签传播矩阵是通过一种闭式解的方法得出的，用于后面生成标签</div><div>loss：用类似softmax函数得出标签，然后采用概率最大化算法，加上负号即可以变成损失函数最小化。</div><div><br/></div><div>实验部分：论文对超参数的选择如k近邻和alpha参数的选择进行了实验分析，同时还考虑加上MAML算法进行实验</div></span>
</div>
<hr>
<a name="694"/>

<div>
<span><div>关系网络采取了一种自动距离度量学习，并非指定某种距离，如欧式距离，余弦距离等等</div><div><img src="few-shot-learning_files/Image [6].png" type="image/png" data-filename="Image.png"/></div><div>一种将元学习和小样本学习结合的方式</div><div>对于one-shot来说，则是将支持样本和测试样本采用统一的特征提取器进行特征提取，然后将提取后的特征进行concatenate，再由一个g网络学习如何分类，即得到测试样本属于各个类的分数，最后采用一种类似于MSE的损失函数进行梯度下降算法，在此之前需要先将测试样本的标签one-hot处理，通过该处理进行损失函数计算。</div><div>对于k-shot来说，则是考虑将每一类的k张图片的feature map图相加在一起，然后再类似于one-shot一样，和测试样本进行concatenate</div><div>对于zero-shot来说，则是采用不同的特征提取器，类如支持集采用f1特征提取器，而测试集采用f2特征提取器，然后将提取后的特征进行拼接</div><div><br/></div><div>对于测试时，我们需要提供支持集和测试集从而进推断，相对于常规深度学习方法这种方式需要更多的计算时间。实验采用的数据集为omniglot数据集，比较小，容易实现</div><div><br/></div><div><span style="font-size: 16px; letter-spacing: normal; orphans: 2; text-indent: 0px; text-transform: none; white-space: normal; widows: 2; word-spacing: 0px; -webkit-text-stroke-width: 0px; color: rgb(25, 25, 25); font-family: &quot;PingFang SC&quot;, Arial, 微软雅黑, 宋体, simsun, sans-serif; font-variant-caps: normal; font-variant-ligatures: normal;">在少样本学习领域，我们的方法可以认为是一种基于度量（metric-based）的方法，但是我们的方法很不一样的一点，也是创新的一点在于我们完全使用神经网络来学习这种度量方式，并且使用元学习的训练方式。而一般的基于度量的方法都是人为的设计一种度量，比如最简单的欧式距离。显然，人为设计的方式总是有缺陷的，那么我们就想来看看，使用神经网络来学习的度量是不是能比人为设计的好。因此，我们做了个小实验来印证这个想法。这个小实验是一个 2 维数据的比较实验。比如这样两个数据（1，2）和（-2，-1），这两个数据看起来是不相关的，但是它们在某一些状态下可能属于同一个类别。那么这种情况，其实传统的人为设计的度量方式实际上就失效了。我们只能通过神经网络去学习这种度量。所以像下图这样复杂的螺旋曲线关系数据情况，我们通过关系网络（relation network）可以学的不错，而人为度量则完全不行。</span></div><div><br/></div></span>
</div>
<hr>
<a name="704"/>

<div>
<span><div>该论文核心思想是用一个循环神经网络或者说LSTM去学习如何优化一个基学习器的参数，例如传统的参数更新是梯度下降等等，现在采用一个网络去学习应该将这个梯度值设置为什么</div><div><img src="few-shot-learning_files/Image [7].png" type="image/png" data-filename="Image.png"/></div><div><img src="few-shot-learning_files/Image [8].png" type="image/png" data-filename="Image.png"/></div><div>算法如上，在每次元学习阶段，采样一批数据集，每一个数据就相当于一个meta-train数据集，然后利用该数据集进行学习，并且得到损失函数，将损失函数传入LSTM结构中，LSTM直接输出基学习器新的参数，等到一批数据处理结束之后，则利用测试集数据对LSTM元学习器参数进行更新，采用梯度下降法或者其变种方法进行更新，相当于完成一次数据迭代，重复多次，从而训练一个良好的LSTM网络结构，并且得出基学习起的参数。</div><div><br/></div></span>
</div>
<hr>
<a name="710"/>

<div>
<span><div>GAN和半监督学习结合可以从一定角度解决数据量不足的问题，并且在此基础上实现一个性能较好的分类器</div><div>通常，将判别器损失和生成器损失分别考虑，进行对抗训练，其中将判别器分类变为k+1类，其中前k类属于有标记样本对应的类，而最后一个k+1类属于虚假样本对应的类</div><div>判别器首先对于有标记样本进行监督学习，而对无标记样本则是将其和噪声生成的数据一起处理，将其分类为k+1类，其中unlabel样本分类k+1类是正确的，而噪声数据分类为k+1是错误的</div><div>生成器损失函数依旧包括两部分，分别是匹配损失和生成能力损失，匹配损失是发生在unlabel样本和噪声样本之间，使噪声数据尽量能够像unlabel标签，从而像真实图片（matching loss也体现了半监督学习的思想），而生成器生成能力损失则是使其生成的样本尽量能够以假乱真，从而降低其分类为k+1的概率，采用1-该概率，再用交叉熵损失函数最小化</div><div><br/></div><div><a href="https://github.com/nejlag/Semi-Supervised-Learning-GAN">https://github.com/nejlag/Semi-Supervised-Learning-GAN</a></div></span>
</div>
<hr>
<a name="712"/>

<div>
<span><div>该论文类似于optimized as a model论文，核心思想还是学习一个元学习器，利用该元学习器去学习如何进行基学习器的参数更新</div><div><img src="few-shot-learning_files/Image [9].png" type="image/png" data-filename="Image.png"/></div><div><br/></div><div>该论文是元学习器给基学习器参数更新信号，即更新的量，而另一篇论文是直接给出新的参数值</div><div><img src="few-shot-learning_files/Image [10].png" type="image/png" data-filename="Image.png"/></div><div>核心思想都是学习如何去进行梯度下降，可以看成通过梯度下降的方式去学习梯度下降，不要简单的认为是一个二阶导问题</div><div><br/></div></span>
</div></body></html> 